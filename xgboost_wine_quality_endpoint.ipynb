{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# はじめに\n",
    "本ハンズオンでは機械学習モデルの解釈性をテーマに、どの特徴量がどの程度の影響があるかについて解析する手法について Amazon SageMaker の組み込みアルゴリズム XGBoost を用いて実践します。\n",
    "\n",
    "## 本ハンズオンで学べる内容\n",
    "- SageMaker の組み込みアルゴリズムである XGBoost を用いた機械学習モデルの学習\n",
    "- SageMaker を用いた際の XGBoost での特徴量重要度の求め方\n",
    "- SageMaker での推論エンドポイントを用いた推論の実施方法\n",
    "- Partianl Dependency Plot(PDP) による特徴量の変化による目的変数への影響の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IAM role\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.session import s3_input\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの準備\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は [UCI Machine Learning Repogitory](https://archive.ics.uci.edu/ml/index.php) にある \"Wine Quality\" というデータセットを活用します。\n",
    "\n",
    "それぞれのワインがアルコール度数や、pH 、残糖量といった計測指標と共に、その品質が 3〜8 で段階的に評価されているデータセットです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"winequality-red.csv\",sep=\";\",encoding=\"utf-8\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各カラムは下記を意味しています。\n",
    "- alcohol: アルコール度数\n",
    "- cholorides: 塩化ナトリウム濃度\n",
    "- citric acid: クエン酸濃度\n",
    "- density: 密度\n",
    "- fixed acidity: 酸性度 \n",
    "- pH: pH\n",
    "- residual sugar: 残糖含有量\n",
    "- sulphates: 硫化カリウム含有量\n",
    "- total sulfur dioxide: 総二酸化硫黄含有量\n",
    "- free sulfur dioxide: 遊離二酸化硫黄含有量\n",
    "- volatile acidity: 揮発性酸度\n",
    "- quality: 品質\n",
    "\n",
    "ヒストグラムを描いて、データの分布を確認しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量をヒストグラムとして可視化\n",
    "hist = data.hist(bins=30, sharey=True, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相関行列を求める\n",
    "データを確認する意味で、それぞれの特徴量同士の相関を見てみましょう。`fixed acidity` と `citric acid` や `density` はやや強い相関が見える、`quality` と強い相関のある特徴量があるとは言えない、などの傾向がつかめます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相関行列を求める\n",
    "display(data.corr())\n",
    "pd.plotting.scatter_matrix(data, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの分割\n",
    "データを学習用、検証用、テスト用データへ分割します。学習と検証用データで構築したモデルに対してテストデータで推論と、その予測値に対してどの特徴量がどの程度寄与しているか、または変化した際に、どの程度影響を及ぼすのかを解析します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMakerの組み込みアルゴリズムへの対応のためデータの並び替え\n",
    "model_data = pd.concat([data['quality'], data.drop(['quality'], axis=1)], axis=1)\n",
    "\n",
    "# 学習用データ、検証用データ、テスト用データへ分割\n",
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])\n",
    "\n",
    "# 学習用データ、検証用データの保存\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)\n",
    "\n",
    "# アップロード先を指定\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'sagemaker/wine-quality-interpretability' \n",
    "\n",
    "# csvファイルとして保存された学習用データ、検証用データをS3バケット上にアップロード\n",
    "sagemaker_session = sagemaker.Session()\n",
    "input_train = sagemaker_session.upload_data(path='train.csv', key_prefix=prefix)\n",
    "input_validation = sagemaker_session.upload_data(path='validation.csv', key_prefix=prefix)\n",
    "\n",
    "s3_input_train = s3_input(s3_data=input_train, content_type='text/csv')\n",
    "s3_input_validation = s3_input(s3_data=input_validation, content_type='text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "今回は SageMaker の組み込みアルゴリズムの中にある XGBoost を使います。まず、XGBoost のコンテナの場所を取得します。コンテナ自体は SageMaker 側で用意されているのでそれを指定します。学習のためにハイパーパラメータや、学習のインスタンスの数やタイプを指定して学習を開始します。XGBoost の hyperparameter に関する詳細は [github](https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst) をご確認下さい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')\n",
    "\n",
    "# 学習ジョブの名前を指定して下さい。XXXXは適宜変更して下さい。\n",
    "job_name=\"xgb-wine-interpretability-XXXX\"\n",
    "\n",
    "# どのような学習インスタンスを使うのかの設定\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "# XGBoostアルゴリズムのハイパラ設定\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        alpha=0.05,\n",
    "                        eta=0.2,\n",
    "                        min_child_weight=2,\n",
    "                        subsample=0.8,\n",
    "                        objective='reg:linear',\n",
    "                        num_round=100)\n",
    "\n",
    "# 学習の開始\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}, job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習させたモデルから特徴量重要度を取得\n",
    "\n",
    "SageMaker で学習されたモデルは s3 のバケットに`model.tar.gz`として保存されています。\n",
    "XGBoost では、どの特徴量がモデルの予測精度の向上に役立ったかは学習時に計算され、`xgboost-model`の中に保存されており、`get_score()` メソッドで呼び出せます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ノートブックインスタンス上に XGBoost のライブラリをインストールすることで、学習済モデルを活用することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge -y xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s3 のバケットから学習済みのモデルをダウンロードし、特徴量重要度をプロットして可視化してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習させたxgboostモデルのダウンロード\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(Bucket=bucket, Key= job_name + '/output/model.tar.gz', Filename = 'model.tar.gz')\n",
    "\n",
    "# xgboostモデルの解凍\n",
    "!tar -zxvf model.tar.gz\n",
    "\n",
    "# ダウンロードしてきたxgboostモデルの読み込み\n",
    "xgb_model = pkl.load(open(\"xgboost-model\", 'rb'))\n",
    "\n",
    "# 特徴量重要度を読み込むためのデータフレームの準備\n",
    "dict_varImp = xgb_model.get_score(importance_type = 'weight')\n",
    "df_ = pd.DataFrame(dict_varImp, index = ['varImp']).transpose().reset_index()\n",
    "df_.columns = ['feature', 'fscore']\n",
    "\n",
    "# 上位10個の特徴量重要度を描写\n",
    "df_['fscore'] = df_['fscore'] / df_['fscore'].max()\n",
    "df_.sort_values('fscore', ascending = False, inplace = True)\n",
    "df_ = df_[0:11]\n",
    "df_.sort_values('fscore', ascending = True, inplace = True)\n",
    "\n",
    "fscore = df_['fscore']\n",
    "feature = df_['feature']\n",
    "\n",
    "# 特徴量重要度が特徴量の名前を保持していないため、データセットのカラム名からマッピング\n",
    "mapper = {'f{0}'.format(i): v for i, v in enumerate(model_data.columns.drop('quality'))}\n",
    "mapped_feature = [mapper[f] for f in df_[\"feature\"]]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('XGBoost Feature Importance Top10', fontsize = 15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.barh(mapped_feature,fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Dependency Plotの活用\n",
    "Partial Dependency Plotは特徴量の寄与度だけでなく、その特徴量の変化した際にどの程度目的変数が変化するかを確認するために活用されます。 手順は下記になります。\n",
    "\n",
    "- 検証データのある点を元にして、影響を見たい1つの特徴量だけを変化させたデータセットを作成する\n",
    "- そのデータに対して学習済モデルで推論をを実施する\n",
    "- 1つの特徴量が変化するとそれぞれのデータ点の予測はどう変わるのかを計算する\n",
    "- 各サンプルの変化や、その平均をプロットする\n",
    "\n",
    "今回は特徴量重要度が高かった `volatile acidity` に対して 0.2 〜 1.0 の間で変化させたデータセットを作成し、それぞれにおいて推論することで Partial Dependency Plotを描いていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論エンドポイントへのデプロイ\n",
    "学習済モデルで推論ができるよう、推論エンドポイントを作成します。deploy()を実行することで、エンドポイントを作成してモデルをデプロイでき、モデルを使った推論ができるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在、エンドポイントをホストしている状態で、これを利用して簡単に予測を行うことができます。予測は http の POST の request を送るだけです。 ここではデータを numpy の array の形式で送って、予測を得られるようにしたいと思います。しかし、endpoint は numpy の array を受け取ることはできません。このために、csv_serializer を利用して、csv 形式に変換して送ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論エンドポイントを活用するためのデータ形式の変換準備\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成済みのテストデータを受け取ると、これをデフォルト500行ずつのデータにわけて、エンドポイントに送信する predict という関数を用意します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを500行ずつ小分けにしてエンドポイントにする関数を準備\n",
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それぞれのサンプルにおいて、`volatile acidity` の値を変化させたデータセットを作成し、推論を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 対象となる特徴量と変化させる値域を設定      \n",
    "target_column = 'volatile acidity'\n",
    "target_range = [round(0.2 + 0.05*x, 2) for x in range(0, 17, 1)]\n",
    "\n",
    "df_plot = pd.DataFrame()\n",
    "\n",
    "for v in target_range:\n",
    "    print(\"各サンプルで、{} が {} だった場合で推論中\".format(target_column, v))\n",
    "    \n",
    "    # 全てのサンプルデータの taeget_column の値が v になったデータセットを作成\n",
    "    pdp_df = data.copy().drop(['quality'], axis=1)\n",
    "    pdp_df[target_column] = v\n",
    "    \n",
    "    # 予測値を格納するための配列を準備\n",
    "    dtest = pdp_df.values\n",
    "    predictions = []\n",
    "    \n",
    "    # 予測の実行\n",
    "    for i in range(dtest.shape[0]):\n",
    "        predictions.append(predict(dtest[i:i+1,:]))\n",
    "    predictions = np.array(predictions).squeeze()\n",
    "    \n",
    "    df_tmp = pd.DataFrame(predictions, columns=[str(v)])\n",
    "    df_plot = pd.concat([df_plot, df_tmp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロットの実行\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Partial Dependency Plot: volatile acidity', fontsize = 15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('volatile acidity', fontsize = 15)\n",
    "plt.ylabel('quality', fontsize = 15)\n",
    "plt.plot(df_plot.T.index,df_plot.T, linewidth=0.2, color='blue')\n",
    "plt.plot(df_plot.mean().index,df_plot.mean(), linewidth=5, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このプロットからは `volatile acidity` の上昇が品質の悪化に緩やかに影響し、特に 0.9 以上になると品質の劣化が著しくなる傾向が見えます。また、それぞれのサンプルでその影響具合が異ることも見てとれます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの削除\n",
    "エンドポイントは起動したままだとコストがかかります。不要な場合は削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
